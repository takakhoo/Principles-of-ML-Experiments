{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 4: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library installation\n",
    "In this assignment we will use the `cvxopt` package, [whose documentation can be found here](https://cvxopt.org/userguide/index.html).\n",
    "\n",
    "We are running version 1.3.2 of `cvxopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxopt in c:\\users\\werer\\.conda\\envs\\engs106_1\\lib\\site-packages (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =======================================================\n",
    "## Import Key Packages\n",
    "## =======================================================\n",
    "## Standard Fare\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "## CVX Opt\n",
    "import cvxopt\n",
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Data\n",
    "You can load the data with `scipy.io.loadmat`, which will return a Python dictionary containing the test and train data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape (1000, 784); test data labels shape (1000, 1)\n",
      "Test data shape (4000, 784); test data labels shape (4000, 1)\n",
      "Labels used:[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "## =======================================================\n",
    "## Import and Manage the MNIST Data\n",
    "## =======================================================\n",
    "## Import the Data\n",
    "mnist = loadmat('MNIST.mat')\n",
    "\n",
    "## Create the test set\n",
    "test_samples = mnist['test_samples']\n",
    "test_samples_labels = mnist['test_samples_labels']\n",
    "print(f'Test data shape {test_samples.shape}; test data labels shape {test_samples_labels.shape}')\n",
    "\n",
    "## Create the training set\n",
    "train_samples = mnist['train_samples']\n",
    "train_samples_labels = mnist['train_samples_labels']\n",
    "print(f'Test data shape {train_samples.shape}; test data labels shape {train_samples_labels.shape}')\n",
    "\n",
    "print(f'Labels used:{np.unique(train_samples_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk06Xdaf8EqO"
   },
   "source": [
    "## Task 1:\n",
    "Develop code for training an SVM for binary classification with nonlinear kernels. You'll need to accomodate non-overlapping class distributions. One way to implement this is to maximize (7.32) subject to (7.33) and (7.34) [in Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf). It may be helpful to redefine these as matrix operations. Let ${1}\\in\\mathbb{R}^{N\\times 1}$ be the vector whose entries are all 1's. Let $\\mathbf{a}\\in\\mathbb{R}^{N\\times 1}$ have entries $a_i$. Let $\\mathbf{T}\\in\\mathbb{R}^{N\\times N}$ be a diagonal matrix with $\\mathbf{T}_{ii} = t_i$ on the diagonal. Then we can reformulate the objective to be\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\text{maximize}\n",
    "& & \\tilde{L}(\\mathbf{a}) = {1}^{\\mathrm{T}}\\mathbf{a} - \\frac{1}{2} \\mathbf{a}^{\\mathrm{T}} \\mathbf{T}\\mathbf{K} \\mathbf{T}\\mathbf{a} \\\\\n",
    "& \\text{subject to}\n",
    "& & {1}^{\\mathrm{T}} \\mathbf{a} \\preceq C \\\\\n",
    "& & & {1}^{\\mathrm{T}} \\mathbf{a} \\succeq 0 \\\\\n",
    "& & & \\mathbf{a}^{\\mathrm{T}} \\mathbf{t} = 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The \"$\\preceq$\" symbol here means element-wise comparison. This formulation is very close to what `cvxopt` expects.\n",
    "\n",
    "Hint (`cvxopt` expects the following form):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\text{minimize}\n",
    "& & \\tilde{L}(\\mathbf{a}) = \\frac{1}{2} \\mathbf{a}^{\\mathrm{T}} \\mathbf{T}\\mathbf{K} \\mathbf{T}\\mathbf{a} - {1}^{\\mathrm{T}}\\mathbf{a} \\\\\n",
    "& \\text{subject to}\n",
    "& & G \\mathbf{a} \\preceq h \\\\\n",
    "& & & {\\mathbf{t}}^{\\mathrm{T}}\\mathbf{a} = 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $G$ is an $N\\times N$ identity matrix ontop of $-1$ times an $N\\times N$ identity matrix and $h \\in\\mathbb{R}^{2N}$ where the first $N$ entries are $C$ and the second $N$ enties are $0$.\n",
    "\n",
    "## Task 2:\n",
    "Develop code to predict the $\\{-1,+1\\}$ class for new data. To use the predictive model (7.13) you need to determine $b$, which can be done with (7.37). \n",
    "\n",
    "We have provided some starter code in the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_kernel(X, y):\n",
    "  \"\"\"\n",
    "  Implement a nonlinear kernel function. Function parameters will vary depending on kernel function.\n",
    "  Feel free to implement either 'rbf', 'poly' or 'sigmoid'\n",
    "  WARNING: 'rbf' can be quite slow and there exist some approximation methods\n",
    "  Parameters\n",
    "  ----------\n",
    "  X : {array-like, sparse matrix} of shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "    Training vectors, where n_samples is the number of samples and n_features \n",
    "    is the number of features. For kernel=”precomputed”, the expected shape \n",
    "    of X is (n_samples, n_samples).\n",
    "\n",
    "  y : array-like of shape (n_samples,)\n",
    "    Target values (class labels in classification, real numbers in regression).\n",
    "  \"\"\"\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SxWx3YK_9sBP"
   },
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "  def __init__(self, kernel=nonlinear_kernel, C=1.0):\n",
    "    \"\"\"\n",
    "    Initialize SVM\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : callable\n",
    "      Specifies the kernel type to be used in the algorithm. It is used to pre-compute \n",
    "      the kernel matrix from data matrices; that matrix should be an array \n",
    "      of shape (n_samples, n_samples).\n",
    "    C : float, default=1.0\n",
    "      Regularization parameter. The strength of the regularization is inversely\n",
    "      proportional to C. Must be strictly positive. The penalty is a squared l2\n",
    "      penalty.\n",
    "    \"\"\"\n",
    "    self.kernel = kernel\n",
    "    self.C = C\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"\n",
    "    Fit the SVM model according to the given training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "      Training vectors, where n_samples is the number of samples and n_features \n",
    "      is the number of features. For kernel=”precomputed”, the expected shape \n",
    "      of X is (n_samples, n_samples).\n",
    "\n",
    "    y : array-like of shape (n_samples,)\n",
    "      Target values (class labels in classification, real numbers in regression).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "      Fitted estimator.\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1. Define Quadratic Programming (QP) parameters. Given a QP optimization \n",
    "    #    problem in standard form, cvxopt is looking for P, q, G, h, A, and b\n",
    "    #    (https://cvxopt.org/userguide/coneprog.html#quadratic-cone-programs).\n",
    "    # 2. Construct the QP, invoke solver (use cvxopt.solvers.qp to maximize the Lagrange (7.32))\n",
    "    # 3. Extract optimal value and solution. cvxopt.solvers.qp(P, q, G, h, A, b)[\"x\"]\n",
    "    #    are the Lagrange multipliers.\n",
    "    return self\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Perform classification on samples in X.\n",
    "\n",
    "    For an one-class model, +1 or -1 is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features) or (n_samples_test, n_samples_train)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "      Class labels for samples in X.\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "  def score(self, X, y):\n",
    "    \"\"\"\n",
    "    Return the mean accuracy on the given test data and labels. \n",
    "    \n",
    "    In multi-label classification, this is the subset accuracy which is a harsh \n",
    "    metric since you require for each sample that each label set be correctly \n",
    "    predicted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "      Test samples.\n",
    "    y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "      True labels for X.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    score : float\n",
    "      Mean accuracy of self.predict(X)\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPYKRw6w9sRi"
   },
   "source": [
    "## Task 3:\n",
    "Using your implementation, compare multiclass classification performance of two different voting schemes:\n",
    "\n",
    "* one versus rest\n",
    "* one versus one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_RwZj21p94bv"
   },
   "outputs": [],
   "source": [
    "## Write and Run Your Own Code Here\n",
    "## Please Add Additional Cells as Needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68yfKC6I93Yr"
   },
   "source": [
    "## Task 4:\n",
    "The parameter $C>0$ controls the tradeoff between the size of the margin and the slack variable penalty. It is analogous to the inverse of a regularization coefficient. Include in your report a brief discussion of how you found an appropriate value.\n",
    "\n",
    "Hint: Try using np.logspace for hyperparameter tuning [[Link]](https://numpy.org/doc/2.1/reference/generated/numpy.logspace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write and Run Your Own Code Here\n",
    "## Please Add Additional Cells as Needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khtnlC9y-Jeu"
   },
   "source": [
    "## Task 5:\n",
    "In addition to calculating percent accuracy, generate multiclass [confusion matrices](https://en.wikipedia.org/wiki/confusion_matrix) as part of your analysis.\n",
    "\n",
    "Hint: Here, you can use sklearn.metrics.confusion_matrix for simpler computation [[Link]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and ConfusionMatrixDisplay [[Link]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XyNW8AAG-UW9"
   },
   "outputs": [],
   "source": [
    "## Write and Run Your Own Code Here\n",
    "## Please Add Additional Cells as Needed.\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Submit\n",
    "Please submit the following:\n",
    "\n",
    "1. Completed notebook: `assignment-4.ipynb`, where the output of each cell is clearly displayed.\n",
    "\n",
    "2. A brief write-up that answers the 5 questions posed in this lab and justifies your model. Ensure that any figures you create are accessible and easy to understand."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN6xvWK6Kjx5ZxD6WXulDDB",
   "collapsed_sections": [],
   "name": "tufts-cs135-spring2022-ps4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "engs106_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
