{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 1: Rock, Paper, Scissors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and Run Your Own Code to Implement a Rock-Paper-Scissors Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RNN for Rock-Paper-Scissors...\n",
      "Collecting 504 steps from the training data...\n",
      "Building winning condition targets from 'store_comp_input'...\n",
      "Training samples: [0. 0. 1. 1. 0. 0.], Validation samples: 101 \n",
      "Epoch 496/500, Training Loss = 0.2757, Validation Loss = 0.0007\n",
      "Epoch 497/500, Training Loss = 0.2753, Validation Loss = 0.0007\n",
      "Epoch 498/500, Training Loss = 0.2750, Validation Loss = 0.0007\n",
      "Epoch 499/500, Training Loss = 0.2747, Validation Loss = 0.0007\n",
      "Epoch 500/500, Training Loss = 0.2744, Validation Loss = 0.0007\n",
      "------------------------------------------------------------------------\n",
      "Training completed after 500 epochs.\n",
      "Decayed learning rate from 0.1 to 0.00043046721\n",
      "Epoch 500/500, Cross Entropy Loss = 0.2744\n",
      "\n",
      "Algorithm recommends: Scissors\n",
      "\n",
      "Calculating number of wins, ties, and losses from training data...\n",
      "Number of Wins: 137\n",
      "Number of Ties: 178\n",
      "Number of Losses: 189\n",
      "Recommended move after training loop: Scissors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "# np.random.seed(0)\n",
    "# random.seed(0)\n",
    "\n",
    "def transfer_function(a):\n",
    "   return np.tanh(a)\n",
    "\n",
    "def transfer_derivative(a):\n",
    "   return 1 - np.tanh(a)**2\n",
    "\n",
    "#Changed to include all weights of theRNN (w_xh, w_hh, w_hy)\n",
    "def loss_function(predicted, target, weights, lambda_reg=1e-5):#Using  Cross Entropy as told to do so by Professor Chin\n",
    "   #Cross entropy better for classification task, diff between prob distributions\n",
    "   epsilon = 1e-12 #preventing the log 0 issue\n",
    "   predicted = np.clip(predicted, epsilon, 1. - epsilon) #for clipping\n",
    "   cross_entropy = -np.mean(target * np.log(predicted)) \n",
    "   l2_penalty = 0.0\n",
    "   for W in weights:\n",
    "       l2_penalty += np.sum(W**2)\n",
    "   l2_loss = (lambda_reg/2)* l2_penalty \n",
    "   return cross_entropy+l2_loss\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits - np.max(logits)) #takin out da outliers to stabilize distr.\n",
    "    return exps / np.sum(exps) #gonna call this softmax later at the cross entropy section\n",
    "\n",
    "class RNN:\n",
    "   #Before we just called this c lass the neuron\n",
    "   def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
    "       self.input_size = input_size\n",
    "       self.hidden_size = hidden_size\n",
    "       self.output_size = output_size\n",
    "       self.lr = lr #learning rate, we set that in arg\n",
    "\n",
    "       #x stands for input and y for otput in gen\n",
    "       #randomizing these w out the fixed seeds as I saw too much repetition when i ran the original\n",
    "       self.weight_xh = np.random.randn(hidden_size, input_size) * 0.01 #Weight (hidden_size x input_size)   [input-to-hidden weights]\n",
    "       self.weight_hh = np.random.randn(hidden_size, hidden_size) * 0.01 #Weight hidden to hidden, basically recurrent weight\n",
    "       self.bias_h  = np.zeros(hidden_size) \n",
    "       self.weight_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "       self.bias_y  = np.zeros(output_size)\n",
    "\n",
    "   def forward_propagate(self, inputs):\n",
    "       T = inputs.shape[0] # T is length of list\n",
    "       hidden_states = []\n",
    "       outputs = []\n",
    "       hidden_state_prev = np.zeros(self.hidden_size)\n",
    "       for t in range(T):\n",
    "           input_t = inputs[t]\n",
    "           activation_t = (self.weight_xh @ input_t) + (self.weight_hh @ hidden_state_prev) + self.bias_h\n",
    "           hidden_t = transfer_function(activation_t) #hidden states\n",
    "\n",
    "           logits = (self.weight_hy @ hidden_t) + self.bias_y #probabilities for softmax\n",
    "           out_t = softmax(logits) #calling the func we made up top\n",
    "           hidden_states.append(hidden_t)\n",
    "           outputs.append(out_t)\n",
    "           hidden_state_prev = hidden_t\n",
    "       return hidden_states, outputs\n",
    "\n",
    "   def back_propagate(self, inputs, hidden_states, outputs, targets, lambda_reg=1e-5):\n",
    "       T = inputs.shape[0] \n",
    "       #all these are to get the gradients from each, let dW = derivative of weight\n",
    "       #same size shape as weight from forwardprop\n",
    "       dW_xh = np.zeros_like(self.weight_xh)\n",
    "       dW_hh = np.zeros_like(self.weight_hh)\n",
    "       db_h  = np.zeros_like(self.bias_h)\n",
    "       dW_hy = np.zeros_like(self.weight_hy)\n",
    "       db_y  = np.zeros_like(self.bias_y)\n",
    "       dh_next = np.zeros(self.hidden_size)  #gradient hidden state from subsequent\n",
    "       total_loss = 0.0\n",
    "       weights = [self.weight_xh, self.weight_hh, self.weight_hy] #for regularizingg\n",
    "\n",
    "       for t in reversed(range(T)):\n",
    "           y_t = outputs[t] #use output variable y from engineering lingo cuz easier for me\n",
    "           h_t = hidden_states[t]\n",
    "           x_t = inputs[t]\n",
    "           if t == 0:\n",
    "               h_prev = np.zeros(self.hidden_size) #work out the zero case like no previous hidden state if 0s\n",
    "           else:\n",
    "               h_prev = hidden_states[t-1]\n",
    "           #gotta get loss w/ reg.; also covers X entropy ofc \n",
    "           total_loss += loss_function(y_t, targets[t], weights, lambda_reg=lambda_reg) \n",
    "           dy = y_t - targets[t]  #(output_size, )\n",
    "           dW_hy += np.outer(dy, h_t) #(output_size, hidden_size))\n",
    "           db_y  += dy #(output_size, )\n",
    "           dh = (self.weight_hy.T @ dy) + dh_next #grad. -> hid layer (hidden_size)\n",
    "           da_t = dh * (1.0 - (h_t**2)) #grad tanh activation\n",
    "           dW_xh += np.outer(da_t, x_t) #get grad wrt input to hidden & hiddenxhidden\n",
    "           dW_hh += np.outer(da_t, h_prev)\n",
    "           db_h  += da_t #grad to hid biass\n",
    "           dh_next = self.weight_hh.T @ da_t #grad -> next itera.\n",
    "       #must add L2 grad to each wt. gradient \n",
    "       dW_xh += lambda_reg * self.weight_xh\n",
    "       dW_hh += lambda_reg * self.weight_hh\n",
    "       dW_hy += lambda_reg * self.weight_hy\n",
    "       #this is gradient descent updates\n",
    "       self.weight_xh -= self.lr * dW_xh\n",
    "       self.weight_hh -= self.lr * dW_hh\n",
    "       self.bias_h  -= self.lr * db_h\n",
    "       self.weight_hy -= self.lr * dW_hy\n",
    "       self.bias_y  -= self.lr * db_y\n",
    "       return total_loss\n",
    "#in these args, dim is diff for each arg (i think?) at diff points\n",
    "def network_init(input_dim, hidden_dim, output_dim, lr=0.1): #simple func initialize RNN class above, wasn;t orig like this\n",
    "   return RNN(input_dim, hidden_dim, output_dim, lr) #easier to work w/\n",
    "\n",
    "def rock_paper_scissor(history, num_epochs=50, lambda_reg=1e-5): #now we can use our history inputs and run through the RNN and plug into this\n",
    "   print(\"Initializing RNN for Rock-Paper-Scissors...\")\n",
    "   input_dim = 6  #3 1-hot for each human nd cpu, respectively = 6\n",
    "   hidden_dim = 50 #50 hiddn neurons for now\n",
    "   output_dim = 3  #obviously (3 options for y)\n",
    "   learning_rate = 1e-3\n",
    "   rnn = network_init(input_dim, hidden_dim, output_dim, lr=learning_rate) #object\n",
    "   steps = len(history)\n",
    "   store_inputs = []\n",
    "   store_comp_input = []\n",
    "   print(f\"Collecting {steps} steps from the training data...\") #user can check if all of data being used\n",
    "\n",
    "   for i in range(steps):\n",
    "       human_move = history[i, 0]\n",
    "       computer_move = history[i, 1]\n",
    "       inp_vec = np.zeros(input_dim) \n",
    "       inp_vec[human_move] = 1\n",
    "       inp_vec[computer_move + 3] = 1\n",
    "       store_inputs.append(inp_vec)\n",
    "       comp_vec = np.zeros(output_dim) #1-hot for cpu move of shape (3,)\\\n",
    "       \"\"\"\n",
    "       Logic if cpu move is 0 we get vector [1,0,0] and so on [0,1,0] or [0,0,1]\n",
    "       \"\"\"\n",
    "       comp_vec[computer_move] = 1\n",
    "       store_comp_input.append(comp_vec)\n",
    "\n",
    "   # Build wincond is the target we built originally and it starts off empty and builds through the network\n",
    "   print(\"Building winning condition targets from 'store_comp_input'...\")\n",
    "   wincond = []\n",
    "   for x in store_comp_input:\n",
    "       if np.array_equal(x, [1, 0, 0]): #cpu rock\n",
    "           wincond.append(np.array([0, 1, 0]))\n",
    "       elif np.array_equal(x, [0, 1, 0]): #cpu paper\n",
    "           wincond.append(np.array([0, 0, 1]))\n",
    "       else:  #cpu scissors\n",
    "           wincond.append(np.array([1, 0, 0]))\n",
    "           #changed the wincond originally from -1,1,0 etc. appending to one-hot's that beat cpu\n",
    "           #mostly because of alteration from MSE to cross entropy \n",
    "   store_inputs = np.array(store_inputs) #(steps,6)\n",
    "   wincond = np.array(wincond) #(steps,3)\n",
    "\n",
    "   train_X, val_X, train_Y, val_Y = train_test_split(store_inputs, wincond, test_size=0.2, shuffle=True, random_state=42)\n",
    "   print(f\"Training samples: {(train_X[0])}, Validation samples: {val_X.shape[0]} \")\n",
    "   patience=10 #num epochs to wait aftr las improvement\n",
    "   best_val_loss = float('inf') #init best valid loss to inf\n",
    "   patience_counter=0 #init patience count\n",
    "   best_weights = None\n",
    "\n",
    "   train_losses=[]\n",
    "   val_losses=[]\n",
    "\n",
    "   # Training loop\n",
    "   for epoch in range(num_epochs):\n",
    "       hidden_states, outputs = rnn.forward_propagate(train_X) #forward pass\n",
    "       total_loss = rnn.back_propagate(train_X, hidden_states, outputs, train_Y, lambda_reg=lambda_reg) #backward paass WITH wincond targets\n",
    "       train_losses.append(total_loss)\n",
    "\n",
    "       #Validation\n",
    "       val_hidden_states, val_outputs = rnn.forward_propagate(val_X)\n",
    "       val_loss = 0.0\n",
    "       for t in range(len(val_X)):\n",
    "           val_loss += loss_function(val_outputs[t], val_Y[t], weights=[rnn.weight_xh, rnn.weight_hh, rnn.weight_hy], lambda_reg=lambda_reg)\n",
    "       val_loss /= len(val_X)\n",
    "       val_losses.append(val_loss)\n",
    "       if(epoch>=num_epochs-5):\n",
    "           print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss = {total_loss:.4f}, Validation Loss = {val_loss:.4f}\") \n",
    "\n",
    "       #Early Stopping\n",
    "       if val_loss<best_val_loss:\n",
    "           best_val_loss=val_loss\n",
    "           patience_counter=0\n",
    "           best_weights = copy.deepcopy({\n",
    "               'weight_xh': rnn.weight_xh.copy(),\n",
    "               'weight_hh': rnn.weight_hh.copy(),\n",
    "               'bias_h': rnn.bias_h.copy(),\n",
    "               'weight_hy': rnn.weight_hy.copy(),\n",
    "               'bias_y': rnn.bias_y.copy()\n",
    "           })\n",
    "           #print(\"Validation loss improved, saving model parameters.\")\n",
    "       else:\n",
    "           patience_counter+=1\n",
    "           print(f\"No improvment in vaidation loss for {patience_counter} epochs\")\n",
    "           if patience_counter >= patience:\n",
    "               print(\"Early stopping triggered.\")\n",
    "               #restoring best model parameters\n",
    "               if best_weights is not None:\n",
    "                   rnn.weight_xh = best_weights['weight_xh']\n",
    "                   rnn.weight_hh = best_weights['weight_hh']\n",
    "                   rnn.bias_h = best_weights['bias_h']\n",
    "                   rnn.weight_hy = best_weights['weight_hy']\n",
    "                   rnn.bias_y = best_weights['bias_y']\n",
    "               break\n",
    "        #LR Decay mess around weith this!!!! during testing\n",
    "       if (epoch +1) % 60 == 0:\n",
    "           rnn.lr *= 0.9  # .5 the learning rate\n",
    "       #if(epoch==180):\n",
    "   print(\"------------------------------------------------------------------------\")\n",
    "   if patience_counter<patience:\n",
    "       print(f\"Training completed after {epoch+1} epochs.\")\n",
    "   else:\n",
    "       print(f\"Training stopped early at epoch {epoch+1}\")\n",
    "   print(f\"Decayed learning rate from 0.1 to {rnn.lr}\")\n",
    "   print(f\"Epoch {epoch+1}/{num_epochs}, Cross Entropy Loss = {total_loss:.4f}\")\n",
    "       \n",
    "   #need one more forward pass\n",
    "   _, outputs_after = rnn.forward_propagate(store_inputs)\n",
    "   final_output = outputs_after[-1] #last timestep\n",
    "   final_output /= np.sum(final_output)\n",
    "\n",
    "   recommended_max = np.random.choice([0,1,2], p=final_output) #making this rec now based on p distr.\n",
    "   moves_map = {0: \"Rock\", 1: \"Paper\", 2: \"Scissors\"}\n",
    "   recommended_move = moves_map[recommended_max]\n",
    "   print(f\"\\nAlgorithm recommends: {recommended_move}\\n\")\n",
    "   \n",
    "   print(\"Calculating number of wins, ties, and losses from training data...\")\n",
    "   wins = 0.0\n",
    "   ties = 0.0\n",
    "   losses = 0.0\n",
    "   for i in range(len(history)):\n",
    "       human_move = history[i, 0]\n",
    "       computer_move = history[i, 1]\n",
    "       if human_move == computer_move:\n",
    "           ties += 1\n",
    "       elif (human_move - computer_move) % 3 == 1:\n",
    "           wins += 1\n",
    "       else:\n",
    "           losses += 1\n",
    "   print(f\"Number of Wins: {wins}\")\n",
    "   print(f\"Number of Ties: {ties}\")\n",
    "   print(f\"Number of Losses: {losses}\")\n",
    "   percentage_win = wins/(wins+ties+losses)\n",
    "   percentage_win1 = wins/(wins+losses)\n",
    "   print\n",
    "   return recommended_move\n",
    "\n",
    "#executing the whole thang!\n",
    "history = np.loadtxt(\"./training.txt\", dtype=int).reshape(-1, 3)\n",
    "recommended = rock_paper_scissor(history, num_epochs=500, lambda_reg=1e-5)\n",
    "print(\"Recommended move after training loop:\", recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Submit?\n",
    "1. A link to your repository (remember to add your Lab TA as a collaborator to this repository, or they won't be able to grade your work!).\n",
    "2. Your training data in training.txt.\n",
    "3. Your model evaluation data.\n",
    "4. A brief write-up describing your algorithm in a couple of sentences, including figures if necessary (your design on paper).\n",
    "5. Finally, please individually briefly reflect on your experience in a couple of sentences. What are you taking away from this lab? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "engs106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
