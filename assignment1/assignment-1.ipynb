{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 1: Rock, Paper, Scissors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and Run Your Own Code to Implement a Rock-Paper-Scissors Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RNN for Rock-Paper-Scissors...\n",
      "Collecting 336 steps from 'history'...\n",
      "Building 'wincond' targets from 'store_comp_input'...\n",
      "Epoch 1/50, MSE Loss = 112.0114\n",
      "Epoch 2/50, MSE Loss = 110.1302\n",
      "Epoch 3/50, MSE Loss = 109.1409\n",
      "Epoch 4/50, MSE Loss = 108.4761\n",
      "Epoch 5/50, MSE Loss = 107.8566\n",
      "Epoch 6/50, MSE Loss = 107.1138\n",
      "Epoch 7/50, MSE Loss = 106.1078\n",
      "Epoch 8/50, MSE Loss = 104.6854\n",
      "Epoch 9/50, MSE Loss = 102.6540\n",
      "Epoch 10/50, MSE Loss = 99.7673\n",
      "Epoch 11/50, MSE Loss = 95.7250\n",
      "Epoch 12/50, MSE Loss = 90.2045\n",
      "Epoch 13/50, MSE Loss = 82.9519\n",
      "Epoch 14/50, MSE Loss = 73.9579\n",
      "Epoch 15/50, MSE Loss = 63.6941\n",
      "Epoch 16/50, MSE Loss = 53.2310\n",
      "Epoch 17/50, MSE Loss = 43.8863\n",
      "Epoch 18/50, MSE Loss = 36.3413\n",
      "Epoch 19/50, MSE Loss = 30.1899\n",
      "Epoch 20/50, MSE Loss = 24.6889\n",
      "Epoch 21/50, MSE Loss = 19.5237\n",
      "Epoch 22/50, MSE Loss = 14.7890\n",
      "Epoch 23/50, MSE Loss = 10.7402\n",
      "Epoch 24/50, MSE Loss = 7.5903\n",
      "Epoch 25/50, MSE Loss = 5.3619\n",
      "Epoch 26/50, MSE Loss = 3.8751\n",
      "Epoch 27/50, MSE Loss = 2.8780\n",
      "Epoch 28/50, MSE Loss = 2.1744\n",
      "Epoch 29/50, MSE Loss = 1.6524\n",
      "Epoch 30/50, MSE Loss = 1.2544\n",
      "Epoch 31/50, MSE Loss = 0.9482\n",
      "Epoch 32/50, MSE Loss = 0.7130\n",
      "Epoch 33/50, MSE Loss = 0.5334\n",
      "Epoch 34/50, MSE Loss = 0.3971\n",
      "Epoch 35/50, MSE Loss = 0.2945\n",
      "Epoch 36/50, MSE Loss = 0.2176\n",
      "Epoch 37/50, MSE Loss = 0.1605\n",
      "Epoch 38/50, MSE Loss = 0.1182\n",
      "Epoch 39/50, MSE Loss = 0.0869\n",
      "Epoch 40/50, MSE Loss = 0.0640\n",
      "Epoch 41/50, MSE Loss = 0.0472\n",
      "Epoch 42/50, MSE Loss = 0.0348\n",
      "Epoch 43/50, MSE Loss = 0.0258\n",
      "Epoch 44/50, MSE Loss = 0.0191\n",
      "Epoch 45/50, MSE Loss = 0.0142\n",
      "Epoch 46/50, MSE Loss = 0.0107\n",
      "Epoch 47/50, MSE Loss = 0.0080\n",
      "Epoch 48/50, MSE Loss = 0.0061\n",
      "Epoch 49/50, MSE Loss = 0.0046\n",
      "Epoch 50/50, MSE Loss = 0.0036\n",
      "\n",
      "Algorithm recommends: Scissors\n",
      "\n",
      "Recommended move after training loop: Scissors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# np.random.seed(0)\n",
    "# random.seed(0)\n",
    "\n",
    "\n",
    "def transfer_function(a):\n",
    "   return np.tanh(-a)\n",
    "\n",
    "\n",
    "def transfer_derivative(a):\n",
    "   return -(1 - np.tanh(-a)**2)\n",
    "\n",
    "\n",
    "def loss_function(predicted, target):#Using MSE as going with Cross Entropy changed entirety of code in frontprop and we had written already\n",
    "   return 0.5 * np.mean((predicted - target)**2)\n",
    "\n",
    "\n",
    "class RNN:\n",
    "   #Before we just called this c lass the neuron\n",
    "   def __init__(self, input_size, hidden_size, output_size, lr=1e-4):\n",
    "       self.input_size = input_size\n",
    "       self.hidden_size = hidden_size\n",
    "       self.output_size = output_size\n",
    "       self.lr = lr #learning rate, we set that in arg\n",
    "\n",
    "\n",
    "       #x stands for input and y for otput in gen\n",
    "       #randomizing these w out the fixed seeds as I saw too much repetition when i ran the original\n",
    "       self.weight_xh = np.random.randn(hidden_size, input_size) * 0.01 #Weight (hidden_size x input_size)   [input-to-hidden weights]\n",
    "       self.weight_hh = np.random.randn(hidden_size, hidden_size) * 0.01 #Weight hidden to hidden, basically recurrent weight\n",
    "       self.bias_h  = np.zeros(hidden_size) #\n",
    "\n",
    "\n",
    "       self.weight_hy = np.random.randn(output_size, hidden_size) * 0.01\n",
    "       self.bias_y  = np.zeros(output_size)\n",
    "\n",
    "\n",
    "   def forward_propagate(self, inputs):\n",
    "       T = inputs.shape[0] # T is length of list\n",
    "       hidden_states = []\n",
    "       outputs = []\n",
    "\n",
    "\n",
    "       hidden_state_prev = np.zeros(self.hidden_size)\n",
    "\n",
    "\n",
    "       for t in range(T):\n",
    "           input_t = inputs[t]\n",
    "           activation_t = (self.weight_xh @ input_t) + (self.weight_hh @ hidden_state_prev) + self.bias_h\n",
    "           hidden_t = transfer_function(activation_t) #hidden states\n",
    "           out_t = (self.weight_hy @ hidden_t) + self.bias_y\n",
    "\n",
    "\n",
    "           hidden_states.append(hidden_t)\n",
    "           outputs.append(out_t)\n",
    "           hidden_state_prev = hidden_t\n",
    "\n",
    "\n",
    "       return hidden_states, outputs\n",
    "\n",
    "\n",
    "   def back_propagate(self, inputs, hidden_states, outputs, targets):\n",
    "       T = inputs.shape[0] #timesteps\n",
    "\n",
    "\n",
    "       #all these are to get the gradients from each, let dW = derivative of weight\n",
    "       #same size shape as weight from forwardprop\n",
    "       dW_xh = np.zeros_like(self.weight_xh)\n",
    "       dW_hh = np.zeros_like(self.weight_hh)\n",
    "       db_h  = np.zeros_like(self.bias_h)\n",
    "       dW_hy = np.zeros_like(self.weight_hy)\n",
    "       db_y  = np.zeros_like(self.bias_y)\n",
    "\n",
    "\n",
    "       dh_next = np.zeros(self.hidden_size)  #gradient hidden state from subsequent\n",
    "       total_loss = 0.0\n",
    "\n",
    "\n",
    "       for t in reversed(range(T)):\n",
    "           y_t = outputs[t] #use output variables from engineering lingo for variable clarification\n",
    "           h_t = hidden_states[t]\n",
    "           x_t = inputs[t]\n",
    "           if t == 0:\n",
    "               h_prev = np.zeros(self.hidden_size) #work out the zero case like no previous hidden state if 0s\n",
    "           else:\n",
    "               h_prev = hidden_states[t - 1]\n",
    "\n",
    "\n",
    "           # MSE loss (instead of cross entropy) at each timestep\n",
    "           total_loss += loss_function(y_t, targets[t])\n",
    "           dy = (y_t - targets[t])  # partial deriv\n",
    "           dW_hy += np.outer(dy, h_t) #back to gradient weight from hidden to output y\n",
    "           db_y  += dy\n",
    "           dh = (self.weight_hy.T @ dy) + dh_next #passing error to hidden layer, use the @ thingy for matrix mult\n",
    "           da_t = dh * (-(1.0 - (h_t**2)))\n",
    "           dW_xh += np.outer(da_t, x_t) #get grad wrt input to hidden & hiddenxhidden\n",
    "           dW_hh += np.outer(da_t, h_prev)\n",
    "           db_h  += da_t #grad to hid biass\n",
    "           dh_next = self.weight_hh.T @ da_t #grad passed further backwards lol hence function\n",
    "\n",
    "\n",
    "       #this is gradient descent all at once, had to do this on paper, not sure if it works lol\n",
    "       self.weight_xh -= self.lr * dW_xh\n",
    "       self.weight_hh -= self.lr * dW_hh\n",
    "       self.bias_h  -= self.lr * db_h\n",
    "       self.weight_hy -= self.lr * dW_hy\n",
    "       self.bias_y  -= self.lr * db_y\n",
    "       return total_loss\n",
    "\n",
    "\n",
    "#in these args, dim is diff for each arg (i think?) at diff points\n",
    "def network_init(input_dim, hidden_dim, output_dim, lr=1e-4): #simple func initialize RNN class above, wasn;t orig like this\n",
    "   return RNN(input_dim, hidden_dim, output_dim, lr) #easier to work w/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rock_paper_scissor(history, num_epochs=50): #now we can use our history inputs and run through the RNN and plug into this\n",
    "   print(\"Initializing RNN for Rock-Paper-Scissors...\")\n",
    "   input_dim = 6  #3 1-hot for each human nd cpu, respectively = 6\n",
    "   hidden_dim = 50 #50 hiddn neurons for now\n",
    "   output_dim = 3  #obviously (3 options for y)\n",
    "   learning_rate = 1e-3\n",
    "   rnn = network_init(input_dim, hidden_dim, output_dim, lr=learning_rate) #object\n",
    "   steps = len(history)\n",
    "   store_inputs = []\n",
    "   store_comp_input = []\n",
    "   print(f\"Collecting {steps} steps from 'history'...\") #user can check if all of data being used\n",
    "\n",
    "\n",
    "   for i in range(steps):\n",
    "       human_move = history[i, 0]\n",
    "       computer_move = history[i, 1]\n",
    "       inp_vec = np.zeros(input_dim) #debugged this forever before realizing needed np here\n",
    "       inp_vec[human_move] = 1\n",
    "       inp_vec[computer_move + 3] = 1\n",
    "       store_inputs.append(inp_vec)\n",
    "       comp_vec = np.zeros(output_dim) #1-hot for cpu move of shape (3,)\\\n",
    "       \"\"\"\n",
    "       Logic if cpu move is 0 we get vector [1,0,0] and so on [0,1,0] or [0,0,1]\n",
    "       \"\"\"\n",
    "       comp_vec[computer_move] = 1\n",
    "       store_comp_input.append(comp_vec)\n",
    "\n",
    "\n",
    "   # Build wincond is the target we built originally and it starts off empty and builds through the network\n",
    "   print(\"Building 'wincond' targets from 'store_comp_input'...\")\n",
    "   wincond = []\n",
    "   for x in store_comp_input:\n",
    "       if np.array_equal(x, [1, 0, 0]): #cpu rock\n",
    "           wincond.append(np.array([ 0,  1, -1]))\n",
    "       elif np.array_equal(x, [0, 1, 0]): #cpu paper\n",
    "           wincond.append(np.array([-1,  0,  1]))\n",
    "       else:  #cpu scissors\n",
    "           wincond.append(np.array([ 1, -1,  0]))\n",
    "\n",
    "\n",
    "   store_inputs = np.array(store_inputs) #TYPING!!!! (steps,6)\n",
    "   wincond = np.array(wincond) #(steps,3)\n",
    "\n",
    "\n",
    "   # Training loop\n",
    "   for epoch in range(num_epochs):\n",
    "       hidden_states, outputs = rnn.forward_propagate(store_inputs) #forward pass\n",
    "       total_loss = rnn.back_propagate(store_inputs, hidden_states, outputs, wincond) #backward paass WITH wincond targets\n",
    "       print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss = {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "   #need one more forward pass\n",
    "   _, outputs_after = rnn.forward_propagate(store_inputs)\n",
    "   final_output = outputs_after[-1] #last timestep\n",
    "   recommended_max_dim = np.argmax(final_output) #largest dimension\n",
    "   moves_map = {0: \"Rock\", 1: \"Paper\", 2: \"Scissors\"}\n",
    "   recommended_move = moves_map[recommended_max_dim]\n",
    "   print(f\"\\nAlgorithm recommends: {recommended_move}\\n\")\n",
    "   return recommended_move\n",
    "\n",
    "\n",
    "#executing the whole thang!\n",
    "history = np.loadtxt(\"./training.txt\", dtype=int).reshape(-1, 3)\n",
    "recommended = rock_paper_scissor(history, num_epochs=50) #arbitrarily decided on 50\n",
    "print(\"Recommended move after training loop:\", recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Submit?\n",
    "1. A link to your repository (remember to add your Lab TA as a collaborator to this repository, or they won't be able to grade your work!).\n",
    "2. Your training data in training.txt.\n",
    "3. Your model evaluation data.\n",
    "4. A brief write-up describing your algorithm in a couple of sentences, including figures if necessary (your design on paper).\n",
    "5. Finally, please individually briefly reflect on your experience in a couple of sentences. What are you taking away from this lab? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "engs106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
